{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment: Анализ изображений\n",
    "\n",
    "Задача заключается в том, чтобы применить предобученную на imagenet нейронную сеть на практической задаче классификации автомобилей. \n",
    "\n",
    "Учиться применять нейронные сети для анализа изображений мы будем на библиотеке TensorFlow. Это известный опенсорсный проект, разработанный инженерами Google Brain Team. Подробнее почитать о TensorFlow можно на официальном сайте, на [гитхабе](https://github.com/tensorflow/tensorflow) или [на хабре](https://habrahabr.ru/post/270543/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Установка окружения\n",
    "\n",
    "В первую очередь нам будет необходимо установить **TensorFlow**.\n",
    "* [Инструкции по установке на сайте](https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#download-and-setup).\n",
    "* Если есть опыт работы с Docker, то можно воспользоваться готовым [докер-контейнером с тензорфлоу](https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#docker-installation).\n",
    "\n",
    "**Важно!** Если вы пользователь Windows, то уставить tensorflow напрямую, к сожалению, не получится:\n",
    "\n",
    "* для пользователей Windows 10 (и выше) нужно использовать **Docker** ([ссылка на дистрибутив](https://www.docker.com/products/docker#/windows));\n",
    "* если у вас Windows старше версии 10, то и вариант с докером не подойдет — он не установится. В таком случае советуем установить линукс на локальную машину как еще одну операционную систему (поможет избежать страданий в будущем при работе с некоторыми библиотеками для ML).\n",
    "\n",
    "Если же поставить Tensorflow на вашу машину никак не получается, мы предлагаем воспользоваться одним из облачных сервисов, в который необходимо установить линукс-образ. Самые популярные облачные сервисы [AWS](https://aws.amazon.com) и [DigitalOcean](http://digitalocean.com) предоставляют бесплатные инстансы (имейте в виду, что для того, чтобы ими воспользоваться, нужно будет привязать кредитную карту).\n",
    "\n",
    "Чтобы освоить компьютерное зрение (или другие интересные задачи из области ML и AI), так или иначе придётся научиться работать с библиотеками нейронных сетей, линуксом и виртуальными серверами. Например, для более масштабных практических задач, крайне необходимы сервера с GPU, а с ними уже локально работать не получиться.\n",
    "\n",
    "Тем не менее, мы понимаем, что в силу временных ограничений курса кто-то может успеть установить TensorFlow. Поэтому мы сделали пункты 1 и 2 необязательными. На оценку они не повлияют — можете сразу переходить к третьему пункту.\n",
    "\n",
    "Помимо tensorflow, потребуется библиотека **`scipy`**. Если вы уже работали с Anaconda и/или выполняли задания в нашей специализации, то она должна присутствовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "Скачать данные нужно тут: https://yadi.sk/d/6m_KbM4HvmLfs \n",
    "\n",
    "Данные это часть выборки _Cars Dataset_ ([link](http://ai.stanford.edu/~jkrause/cars/car_dataset.html)). Исходный датасет содержит 16,185 изображений автомобилей, принадлежащих к 196 классам. Данные разделены на 8,144 тренировочных и 8,041 тестовых изображений, при этом каждый класс разделён приблизительно поровну между тестом и трейном. Все классы уровня параметров _Марка_, _Год_, _Модель_ и др. (например, _2012 Tesla Model S or 2012 BMW M3 coupe_).\n",
    "\n",
    "В нашем же случае в `train` 204 изображения, и в `test` — 202 изображения.\n",
    "\n",
    "## Что делать\n",
    "\n",
    "Помимо данных, потребуется скачать:\n",
    "\n",
    "* [код](https://github.com/ton4eg/coursera_pa), \n",
    "* веса модели [по ссылке](https://yadi.sk/d/9-3kXyxRvnBwh)\n",
    "\n",
    "Положите данные, код и модель в одну папку. У вас должна получиться такая структура:\n",
    "\n",
    "```\n",
    "/assignment-computer-vision/\n",
    "|\n",
    "|-- test              # папки  \n",
    "|    `---- ...        # с\n",
    "|-- train             # картинками\n",
    "|    `---- ...\n",
    "|\n",
    "|-- class_names.txt   # имена классов, номер строки соответствует id класса\n",
    "|-- results.txt       # соответствие имя картинки — id класса\n",
    "|-- vgg16_weights.npz # веса модели в формате tensorflow\n",
    "|\n",
    "|-- vgg16.py            # основной скрипт\n",
    "|-- imagenet_classes.py \n",
    "|\n",
    "`-- beach.jpg         # картиночка с пляжем\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск из Docker gcr.io/tensorflow/tensorflow\n",
    "* докер запускает Jupyter Notebook с рабочей папкой /notebooks\n",
    "* в докере не хватает библиотек sklearn и Pillow\n",
    "\n",
    "Рекомендуется запускать докер командой\n",
    "\n",
    "docker run -it -p 127.0.0.1:8888:8888 -v $PWD:/notebooks gcr.io/tensorflow/tensorflow\n",
    "\n",
    "при запуске из папки, где лежит данный ноутбук и все нужные файлы, они сразу окажутся в рабочей папке Jupyter \n",
    "Notebook.\n",
    "\n",
    "Следующие две ячейки содержат команды, доустанавливающие нужные библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/5d/6a/9669836f813b73fe5abf5e9f118ccc9b7fb060f02789d385825b0943f9c8/tensorflow-2.3.1-cp37-cp37m-win_amd64.whl (342.5MB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\arsty\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Collecting tensorboard<3,>=2.3.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/1b/6a420d7e6ba431cf3d51b2a5bfa06a958c4141e3189385963dc7f6fbffb6/tensorboard-2.3.0-py3-none-any.whl (6.8MB)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/67/5f/bf822211f7f94a2f6d0f8fd3bda3b804d7b24b6d5c84dbc6e6c9df4c74c2/grpcio-1.32.0-cp37-cp37m-win_amd64.whl (2.5MB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\arsty\\anaconda3\\lib\\site-packages (from tensorflow) (0.33.4)\n",
      "Collecting h5py<2.11.0,>=2.10.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/6b/7f62017e3f0b32438dd90bdc1ff0b7b1448b6cb04a1ed84f37b6de95cd7b/h5py-2.10.0-cp37-cp37m-win_amd64.whl (2.5MB)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\users\\arsty\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.4)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\arsty\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Collecting gast==0.3.3 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
      "Collecting astunparse==1.6.3 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\n",
      "Collecting google-pasta>=0.1.8 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting protobuf>=3.9.2 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/6b/2e/28425c709c26525998be0b7a91c4090c87c38a1a9644fd43fefaea2e16c0/protobuf-3.13.0-cp37-cp37m-win_amd64.whl (1.0MB)\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/07/f69dd3367368ad69f174bfe426a973651412ec11d48ec05c000f19fe0561/absl_py-0.10.0-py3-none-any.whl (127kB)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<3,>=2.3.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/34/4d6b7e3044044e89eaa25ed5395656cc351163c625fda0656d2729de399f/Markdown-3.3.2-py3-none-any.whl (95kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\arsty\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (41.0.1)\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard<3,>=2.3.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/cf/724b6436967a8be879c8de16b09fd80e0e7b0bcad462f5c09ee021605785/google_auth-1.22.1-py2.py3-none-any.whl (114kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<3,>=2.3.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/b6/85/5c5ac0a8c5efdfab916e9c6bc18963f6a6996a8a1e19ec4ad8c9ac9c623c/tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<3,>=2.3.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\arsty\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\arsty\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.15.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\arsty\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (0.17)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\" (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/df/c3587a667d6b308fadc90b99e8bc8774788d033efcc70f4ecaae7fad144b/rsa-4.6-py3-none-any.whl (47kB)\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/5c/f3aa86b6d5482f3051b433c7616668a9b96fbe49a622210e2c9781938a5c/cachetools-4.1.1-py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arsty\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\arsty\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\arsty\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\arsty\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\arsty\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (0.5.1)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\arsty\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built termcolor\n",
      "Installing collected packages: absl-py, markdown, grpcio, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, tensorboard-plugin-wit, protobuf, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, h5py, gast, astunparse, google-pasta, opt-einsum, termcolor, keras-preprocessing, tensorflow-estimator, tensorflow\n",
      "  Found existing installation: h5py 2.9.0\n",
      "    Uninstalling h5py-2.9.0:\n",
      "      Successfully uninstalled h5py-2.9.0\n",
      "Successfully installed absl-py-0.10.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.22.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.3.2 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/65/e4a5130b4162d20ed99ff096549a04d18f050cfcdb16fe1643ac751c0181/Pillow-8.0.1-cp37-cp37m-win_amd64.whl (2.1MB)\n",
      "Installing collected packages: pillow\n",
      "  Found existing installation: Pillow 8.0.0\n",
      "    Uninstalling Pillow-8.0.0:\n",
      "      Successfully uninstalled Pillow-8.0.0\n",
      "Successfully installed pillow-8.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем всё, что нам нужно для работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda create -n tf python=3.7 jupyter=1.0.0 tensorflow=2.1.0 scikit-learn=0.22.1 scikit-image=0.16.2 imageio=2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\arsty\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# inspired by\n",
    "# http://www.cs.toronto.edu/~frossard/post/vgg16/                               \n",
    "# Model from https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md     #\n",
    "# Weights from Caffe converted using https://github.com/ethereon/caffe-tensorflow      #\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "import numpy as np\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "from imagenet_classes import class_names\n",
    "import sys\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом классе содержится описание модели VGG - структура, инициализация, загрузка весов. Следует помнить - пока не запущена сессия Tensorflow, никакой реальной работы не производится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg16:\n",
    "    def __init__(self, imgs, weights=None, sess=None):\n",
    "        self.imgs = imgs\n",
    "        self.convlayers()\n",
    "        self.fc_layers()\n",
    "        self.probs = tf.nn.softmax(self.fc3l)\n",
    "        if weights is not None and sess is not None:\n",
    "            self.load_weights(weights, sess)\n",
    "\n",
    "\n",
    "    def convlayers(self):\n",
    "        self.parameters = []\n",
    "\n",
    "        # zero-mean input\n",
    "        with tf.name_scope('preprocess') as scope:\n",
    "            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')\n",
    "            images = self.imgs-mean\n",
    "\n",
    "        # conv1_1\n",
    "        with tf.name_scope('conv1_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv1_2\n",
    "        with tf.name_scope('conv1_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool1\n",
    "        self.pool1 = tf.nn.max_pool(self.conv1_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool1')\n",
    "\n",
    "        # conv2_1\n",
    "        with tf.name_scope('conv2_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv2_2\n",
    "        with tf.name_scope('conv2_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool2\n",
    "        self.pool2 = tf.nn.max_pool(self.conv2_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool2')\n",
    "\n",
    "        # conv3_1\n",
    "        with tf.name_scope('conv3_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_2\n",
    "        with tf.name_scope('conv3_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_3\n",
    "        with tf.name_scope('conv3_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool3\n",
    "        self.pool3 = tf.nn.max_pool(self.conv3_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool3')\n",
    "\n",
    "        # conv4_1\n",
    "        with tf.name_scope('conv4_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_2\n",
    "        with tf.name_scope('conv4_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_3\n",
    "        with tf.name_scope('conv4_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool4\n",
    "        self.pool4 = tf.nn.max_pool(self.conv4_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "        # conv5_1\n",
    "        with tf.name_scope('conv5_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool4, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_2\n",
    "        with tf.name_scope('conv5_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_3\n",
    "        with tf.name_scope('conv5_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool5\n",
    "        self.pool5 = tf.nn.max_pool(self.conv5_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "    def fc_layers(self):\n",
    "        # fc1\n",
    "        with tf.name_scope('fc1') as scope:\n",
    "            shape = int(np.prod(self.pool5.get_shape()[1:]))\n",
    "            fc1w = tf.Variable(tf.truncated_normal([shape, 4096],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            pool5_flat = tf.reshape(self.pool5, [-1, shape])\n",
    "            fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)\n",
    "            self.fc1 = tf.nn.relu(fc1l)\n",
    "            self.parameters += [fc1w, fc1b]\n",
    "\n",
    "        # fc2\n",
    "        with tf.name_scope('fc2') as scope:\n",
    "            fc2w = tf.Variable(tf.truncated_normal([4096, 4096],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            fc2l = tf.nn.bias_add(tf.matmul(self.fc1, fc2w), fc2b)\n",
    "            self.fc2 = tf.nn.relu(fc2l)\n",
    "            self.parameters += [fc2w, fc2b]\n",
    "\n",
    "        # fc3\n",
    "        with tf.name_scope('fc3') as scope:\n",
    "            fc3w = tf.Variable(tf.truncated_normal([4096, 1000],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc3b = tf.Variable(tf.constant(1.0, shape=[1000], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            self.fc3l = tf.nn.bias_add(tf.matmul(self.fc2, fc3w), fc3b)\n",
    "            self.parameters += [fc3w, fc3b]\n",
    "\n",
    "    def load_weights(self, weight_file, sess):\n",
    "        weights = np.load(weight_file)\n",
    "        keys = sorted(weights.keys())\n",
    "        for i, k in enumerate(keys):\n",
    "            print(i, k, np.shape(weights[k]))\n",
    "            sess.run(self.parameters[i].assign(weights[k]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция сохранения в файл ответа, состоящего из одного числа\n",
    "def save_answerNum(fname,number):\n",
    "    with open(fname,\"w\") as fout:\n",
    "        fout.write(str(number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция сохранения в файл ответа, представленного массивом\n",
    "def save_answerArray(fname,array):\n",
    "    with open(fname,\"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in array]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка словаря из текстового файла. Словарь у нас используется для сохранения меток классов в выборке data.\n",
    "def load_txt(fname):\n",
    "    line_dict = {}\n",
    "    for line in open(fname):\n",
    "        fname, class_id = line.strip().split()\n",
    "        line_dict[fname] = class_id\n",
    "\n",
    "    return line_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция обработки отдельного изображения, печатает метки TOP-5 классов и уверенность модели в каждом из них.\n",
    "def process_image(fname):\n",
    "    img1 = imread(fname, pilmode='RGB')\n",
    "    img1 = resize(img1, (224, 224))\n",
    "    \n",
    "    prob = sess.run(vgg.probs, feed_dict={vgg.imgs: [img1]})[0]\n",
    "    preds = (np.argsort(prob)[::-1])[0:5]\n",
    "    for p in preds:\n",
    "        print(class_names[p], prob[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv1_1_W (3, 3, 3, 64)\n",
      "1 conv1_1_b (64,)\n",
      "2 conv1_2_W (3, 3, 64, 64)\n",
      "3 conv1_2_b (64,)\n",
      "4 conv2_1_W (3, 3, 64, 128)\n",
      "5 conv2_1_b (128,)\n",
      "6 conv2_2_W (3, 3, 128, 128)\n",
      "7 conv2_2_b (128,)\n",
      "8 conv3_1_W (3, 3, 128, 256)\n",
      "9 conv3_1_b (256,)\n",
      "10 conv3_2_W (3, 3, 256, 256)\n",
      "11 conv3_2_b (256,)\n",
      "12 conv3_3_W (3, 3, 256, 256)\n",
      "13 conv3_3_b (256,)\n",
      "14 conv4_1_W (3, 3, 256, 512)\n",
      "15 conv4_1_b (512,)\n",
      "16 conv4_2_W (3, 3, 512, 512)\n",
      "17 conv4_2_b (512,)\n",
      "18 conv4_3_W (3, 3, 512, 512)\n",
      "19 conv4_3_b (512,)\n",
      "20 conv5_1_W (3, 3, 512, 512)\n",
      "21 conv5_1_b (512,)\n",
      "22 conv5_2_W (3, 3, 512, 512)\n",
      "23 conv5_2_b (512,)\n",
      "24 conv5_3_W (3, 3, 512, 512)\n",
      "25 conv5_3_b (512,)\n",
      "26 fc6_W (25088, 4096)\n",
      "27 fc6_b (4096,)\n",
      "28 fc7_W (4096, 4096)\n",
      "29 fc7_b (4096,)\n",
      "30 fc8_W (4096, 1000)\n",
      "31 fc8_b (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Инициируем TF сессию, и инициализируем модель. На этом шаге модель загружает веса. Веса - это 500Мб в сжатом виде\n",
    "# и ~2.5Гб в памяти, процесс их загрузки послойно выводится ниже этой ячейки, и если вы увидите этот вывод ещё раз - \n",
    "# у вас неистово кончается память. Остановитесь. Также, не запускайте эту ячейку на выполнение больше одного раза\n",
    "# за запуск ядра Jupyter.\n",
    "sess = tf.Session()\n",
    "imgs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "vgg = vgg16(imgs, 'vgg16_weights.npz', sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все ячейки выше не нуждаются в модификации для выполнения задания, и необходимы к исполнению только один раз, в порядке следования. Повторный запуск ячейки с инициализацией модели будет сжирать память. Вы предупреждены."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1\\. \n",
    "\n",
    "Для начала нужно запустить готовую модель `vgg16`, предобученную на `imagenet`. Модель обучена с помощью `caffe` и сконвертирована в формат `tensorflow` - `vgg16_weights.npz`. Скрипт, иллюстрирующий применение этой модели к изображению, возвращает топ-5 классов из `imagenet` и уверенность в этих классах.\n",
    "\n",
    "**Задание:** Загрузите уверенность для первого класса для изображения `train/00002.jpg` с точностью до 1 знака после запятой в файл с ответом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minivan 0.4000311\n",
      "beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon 0.11736069\n",
      "sports car, sport car 0.050051834\n",
      "car wheel 0.0453139\n",
      "grille, radiator grille 0.032082383\n"
     ]
    }
   ],
   "source": [
    "# Ваш код здесь\n",
    "img1 = np.load('train/00002_224x224.npy', allow_pickle=True, encoding='bytes')\n",
    "\n",
    "    \n",
    "prob = sess.run(vgg.probs, feed_dict={vgg.imgs: [img1]})[0]\n",
    "preds = (np.argsort(prob)[::-1])[0:5]\n",
    "for p in preds:\n",
    "    print(class_names[p], prob[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answerNum('vgg16_answer.txt',round(prob[preds[0]],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2\\. \n",
    "\n",
    "Научитесь извлекать `fc2` слой. Возьмите за основу код `process_image`, и модифицируйте, чтобы вместо последнего слоя извлекались выходы `fc2`.\n",
    "\n",
    "**Задание:** Посчитайте `fc2` для картинки `train/00002.jpg`.  Запишите первые 20 компонент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 3.87281895e-01, 2.26503193e-01, 9.10454750e-01,\n",
       "       1.17599964e-04, 0.00000000e+00, 1.11121774e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.40186906e-01, 0.00000000e+00,\n",
       "       2.30205536e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = imread('train/00002.jpg', pilmode='RGB')\n",
    "img1 = resize(img1, (224, 224))\n",
    "    \n",
    "prob = sess.run(vgg.fc2, feed_dict={vgg.imgs: [img1]})[0]\n",
    "prob[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answerArray(\"vgg16_answer2.txt\",prob[0:20] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3\\. \n",
    "\n",
    "Теперь необходимо дообучить классификатор на нашей базе. В качестве бейзлайна предлагается воспользоваться классификатором `svm` из пакета `scipy`.\n",
    "\n",
    "- Модифицировать функцию `get_features` и добавить возможность вычислять `fc2`. (Аналогично второму заданию).\n",
    "- Применить `get_feautures`, чтобы получить `X_test` и `Y_test`.\n",
    "- Воспользоваться классификатором `SVC` с `random_state=0`.\n",
    "\n",
    "> **Важно!** Если вам не удалось поставить `tensorflow`, то необходимо вместо использования функции `get_features`, загрузить предпосчитанные `X`, `Y`, `X_test`, `Y_test` из архива: https://yadi.sk/d/RzMOK8Fjvs6Ln и воспользоваться функцией `np.load` для их загрузки, а после этого два последних пункта.\n",
    "\n",
    "**Задание:** Сколько правильных ответов получается на валидационной выборке из папки `test`? Запишите в файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=SVC(gamma='auto',random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prepared():\n",
    "    X=np.load('X.npy', allow_pickle=True, encoding='bytes')\n",
    "    Y=np.load('Y.npy', allow_pickle=True, encoding='bytes')\n",
    "    X_test=np.load('X_test.npy', allow_pickle=True, encoding='bytes')\n",
    "    Y_test=np.load('Y_test.npy', allow_pickle=True, encoding='bytes')\n",
    "    clf.fit(X,Y)\n",
    "    \n",
    "    Y_pred=clf.predict(X_test)\n",
    "    return sum(Y_test==Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=process_prepared()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answerNum('vgg16_answer3.txt',res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
